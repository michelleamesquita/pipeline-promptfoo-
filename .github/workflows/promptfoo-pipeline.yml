name: promptfoo-pipeline

on:
  workflow_dispatch:

jobs:
  generate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install promptfoo
        run: npm install -g promptfoo@latest

      - name: Generate red team tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          cd "$GITHUB_WORKSPACE"
          promptfoo redteam generate --force --config ./teste.yml --output ./redteam-tests.yml

      - name: Upload red team tests
        uses: actions/upload-artifact@v4
        with:
          name: redteam-tests
          path: |
            redteam-tests.yml

  evaluate:
    runs-on: ubuntu-latest
    needs: [generate]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install promptfoo
        run: npm install -g promptfoo@latest

      - name: Download red team tests
        uses: actions/download-artifact@v4
        with:
          name: redteam-tests

      - name: Evaluate plugin (redteam eval)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          cd "$GITHUB_WORKSPACE"
          promptfoo redteam eval -c ./redteam-tests.yml --output ./redteam-results.json

      - name: Evaluate with LLM-as-judge
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          cd "$GITHUB_WORKSPACE"
          promptfoo eval -c ./redteam-tests.yml --output ./llm-judge-results.json

      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            redteam-results.json
            llm-judge-results.json

  summarize:
    runs-on: ubuntu-latest
    needs: [evaluate]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install pyyaml

      - name: Download red team tests
        uses: actions/download-artifact@v4
        with:
          name: redteam-tests

      - name: Download evaluation results
        uses: actions/download-artifact@v4
        with:
          name: evaluation-results

      - name: Add prompt metrics
        run: |
          cd "$GITHUB_WORKSPACE"
          python3 add_prompt_metrics.py

      - name: Summarize scores
        run: |
          cd "$GITHUB_WORKSPACE"
          python3 summarize_scores.py
          echo "----- score_summary.md -----"
          cat score_summary.md

      - name: Upload summary outputs
        uses: actions/upload-artifact@v4
        with:
          name: score-summary
          path: |
            prompt_metrics.json
            score_summary.md

  human_approval:
    runs-on: ubuntu-latest
    needs: [summarize]
    environment: human-approval
    steps:
      - name: Manual validation
        run: |
          echo "âœ… Human-in-the-loop: modelos validados e aceitos."
